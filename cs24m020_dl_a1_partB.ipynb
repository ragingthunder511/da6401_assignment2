{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "cs24m020_dl_a1_partB",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ragingthunder511/da6401_assignment2/blob/main/cs24m020_dl_a1_partB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Imports --------------------\n",
        "import wandb\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "\n",
        "# -------------------- Weights & Biases Setup --------------------\n",
        "wandb.login(key=\"01bb56b62b8d93215a878ebdbc41b79e456d010c\")\n",
        "\n",
        "# -------------------- Dataset Download & Extraction --------------------\n",
        "os.system(\"curl -L -o data.zip https://storage.googleapis.com/wandb_datasets/nature_12K.zip\")\n",
        "os.system(\"unzip -qq data.zip && rm data.zip\")\n",
        "\n",
        "# -------------------- Configuration --------------------\n",
        "TUNED_PARAMS = {\n",
        "    'weight_decay': 0,\n",
        "    'learning_rate': 1e-4,\n",
        "    'dropout': 0.2,\n",
        "    'activation': 'relu',\n",
        "    'optimiser': 'rmsprop',\n",
        "    'batch_norm': 'true',\n",
        "    'batch_size': 32,\n",
        "    'dense_layer': 256\n",
        "}\n",
        "\n",
        "wandb.init(project='cs24m020_dl_a2_partB', config=TUNED_PARAMS)\n",
        "cfg = wandb.config\n",
        "\n",
        "# -------------------- Data Utility --------------------\n",
        "def build_transforms(train=True):\n",
        "    norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    if train:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.RandomRotation(15),  # Slight rotation added\n",
        "            transforms.RandomAffine(10),    # Minor affine transformation added\n",
        "            transforms.ToTensor(),\n",
        "            norm\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.CenterCrop(224),  # Center crop during validation\n",
        "            transforms.ToTensor(),\n",
        "            norm\n",
        "        ])\n",
        "\n",
        "def load_data(batch_size):\n",
        "    train_val = datasets.ImageFolder('inaturalist_12K/train', transform=build_transforms(True))\n",
        "    train_size = int(0.8 * len(train_val))\n",
        "    val_size = len(train_val) - train_size\n",
        "    train_set, val_set = random_split(train_val, [train_size, val_size])\n",
        "    val_set.dataset.transform = build_transforms(False)\n",
        "\n",
        "    test_set = datasets.ImageFolder('inaturalist_12K/val', transform=build_transforms(False))\n",
        "\n",
        "    return (\n",
        "        DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
        "        DataLoader(val_set, batch_size=batch_size),\n",
        "        DataLoader(test_set, batch_size=batch_size)\n",
        "    )\n",
        "\n",
        "# -------------------- Custom Activation Function --------------------\n",
        "class CustomActivation(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.maximum(x, torch.zeros_like(x))  # A custom LeakyReLU variant\n",
        "\n",
        "def get_activation_fn(name):\n",
        "    if name == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif name == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    elif name == 'custom':  # Custom activation used\n",
        "        return CustomActivation()\n",
        "    return nn.Sigmoid()\n",
        "\n",
        "# -------------------- Model Builder --------------------\n",
        "def build_custom_head(input_dim, hidden_dim, dropout, bn_flag, act):\n",
        "    layers = [nn.Dropout(p=dropout), nn.Linear(input_dim, hidden_dim)]\n",
        "    if bn_flag == 'true':\n",
        "        layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "    layers.append(get_activation_fn(act))\n",
        "    layers.append(nn.Linear(hidden_dim, 512))  # Added extra layer for complexity\n",
        "    layers.append(nn.ReLU())\n",
        "    layers.append(nn.Linear(512, 10))  # Output layer for 10 classes\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# -------------------- Fine-Tune Engine --------------------\n",
        "class ModelRefiner:\n",
        "    def __init__(self, config):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self._setup_model(config)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.optimizer, self.scheduler = self._get_optimizer(config)\n",
        "\n",
        "    def _setup_model(self, config):\n",
        "        base = models.resnet50(pretrained=True)\n",
        "        for param in base.parameters():\n",
        "            param.requires_grad = False\n",
        "        base.fc = build_custom_head(\n",
        "            base.fc.in_features,\n",
        "            config.dense_layer,\n",
        "            config.dropout,\n",
        "            config.batch_norm,\n",
        "            config.activation\n",
        "        )\n",
        "        return base.to(self.device)\n",
        "\n",
        "    def _get_optimizer(self, config):\n",
        "        head_params = self.model.fc.parameters()\n",
        "        opt = config.optimiser\n",
        "        if opt == 'adam':\n",
        "            optimizer = optim.Adam(head_params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "        elif opt == 'rmsprop':\n",
        "            optimizer = optim.RMSprop(head_params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "        else:\n",
        "            optimizer = optim.SGD(head_params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "\n",
        "        # StepLR scheduler\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "        return optimizer, scheduler\n",
        "\n",
        "    def _process_epoch(self, loader, training=True):\n",
        "        mode = self.model.train if training else self.model.eval\n",
        "        mode()\n",
        "        torch.set_grad_enabled(training)\n",
        "\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            if training: self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss_fn(outputs, targets)\n",
        "\n",
        "            if training:\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "        if training:\n",
        "            self.scheduler.step()  # Adjust the learning rate schedule\n",
        "\n",
        "        return total_loss / len(loader), 100 * correct / total\n",
        "\n",
        "    def train_and_evaluate(self, train_loader, val_loader, epochs):\n",
        "        for ep in range(1, epochs + 1):\n",
        "            tr_loss, tr_acc = self._process_epoch(train_loader, training=True)\n",
        "            vl_loss, vl_acc = self._process_epoch(val_loader, training=False)\n",
        "\n",
        "            print(f\"Epoch {ep} | Train Loss: {tr_loss:.4f} | Acc: {tr_acc:.2f}% | Val Loss: {vl_loss:.4f} | Acc: {vl_acc:.2f}%\")\n",
        "            wandb.log({\n",
        "                'epoch': ep,\n",
        "                'train_loss': tr_loss,\n",
        "                'train_accuracy': tr_acc,\n",
        "                'val_loss': vl_loss,\n",
        "                'val_accuracy': vl_acc,\n",
        "                'lr': self.optimizer.param_groups[0]['lr']  # Log learning rate\n",
        "            })\n",
        "\n",
        "    def evaluate_on_test(self, test_loader):\n",
        "        loss, acc = self._process_epoch(test_loader, training=False)\n",
        "        print(f\"\\nTest Loss: {loss:.4f} | Test Accuracy: {acc:.2f}%\")\n",
        "        wandb.log({'test_loss': loss, 'test_accuracy': acc})\n",
        "        return loss, acc\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        torch.save(self.model.state_dict(), filename)\n",
        "        wandb.save(filename)\n",
        "\n",
        "# -------------------- Run Training --------------------\n",
        "train_loader, val_loader, test_loader = load_data(cfg.batch_size)\n",
        "engine = ModelRefiner(cfg)\n",
        "engine.train_and_evaluate(train_loader, val_loader, epochs=10)\n",
        "engine.evaluate_on_test(test_loader)\n",
        "engine.save_model(\"refined_resnet50.pth\")\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T20:09:09.262449Z",
          "iopub.execute_input": "2025-04-19T20:09:09.262683Z"
        },
        "id": "YgxhBCu4vXjZ",
        "outputId": "fd1fe931-8e8e-4e26-bd65-2257c43d41ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarekargrishma1234\u001b[0m (\u001b[33mkarekargrishma1234-iit-madras-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 3639M  100 3639M    0     0   214M      0  0:00:16  0:00:16 --:--:--  214M\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.19.6"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_201009-362m8n6r</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_partB/runs/362m8n6r' target=\"_blank\">leafy-energy-6</a></strong> to <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_partB' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_partB' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_partB</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_partB/runs/362m8n6r' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_partB/runs/362m8n6r</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 196MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1 | Train Loss: 1.2083 | Acc: 64.88% | Val Loss: 0.8951 | Acc: 73.90%\nEpoch 2 | Train Loss: 0.9319 | Acc: 71.21% | Val Loss: 0.8198 | Acc: 74.05%\nEpoch 3 | Train Loss: 0.8595 | Acc: 72.20% | Val Loss: 0.7643 | Acc: 75.25%\nEpoch 4 | Train Loss: 0.8030 | Acc: 74.02% | Val Loss: 0.7556 | Acc: 75.10%\nEpoch 5 | Train Loss: 0.7797 | Acc: 74.32% | Val Loss: 0.7465 | Acc: 75.45%\nEpoch 6 | Train Loss: 0.7643 | Acc: 74.77% | Val Loss: 0.7316 | Acc: 76.00%\nEpoch 7 | Train Loss: 0.7396 | Acc: 75.40% | Val Loss: 0.7253 | Acc: 75.90%\nEpoch 8 | Train Loss: 0.7267 | Acc: 75.72% | Val Loss: 0.7190 | Acc: 76.10%\nEpoch 9 | Train Loss: 0.7080 | Acc: 76.15% | Val Loss: 0.7064 | Acc: 77.45%\nEpoch 10 | Train Loss: 0.6965 | Acc: 76.48% | Val Loss: 0.7175 | Acc: 76.85%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "mCczxEGjvXjh"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}