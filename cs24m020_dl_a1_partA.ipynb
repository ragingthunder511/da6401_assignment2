{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "cs24m020_dl_a1_partA",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ragingthunder511/da6401_assignment2/blob/main/cs24m020_dl_a1_partA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from typing import List, Tuple\n",
        "from torch.cuda.amp import autocast, GradScaler\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T16:14:54.067432Z",
          "iopub.execute_input": "2025-04-19T16:14:54.067731Z",
          "iopub.status.idle": "2025-04-19T16:15:01.992162Z",
          "shell.execute_reply.started": "2025-04-19T16:14:54.067707Z",
          "shell.execute_reply": "2025-04-19T16:15:01.991578Z"
        },
        "id": "lfarIh4VFXgg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#WandB authentication\n",
        "wandb.login(key=\"01bb56b62b8d93215a878ebdbc41b79e456d010c\")\n",
        "#Downloading the iNaturalist dataset\n",
        "!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n",
        "!unzip -q nature_12K.zip"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T17:04:42.620284Z",
          "iopub.execute_input": "2025-04-19T17:04:42.620831Z",
          "iopub.status.idle": "2025-04-19T17:05:24.508371Z",
          "shell.execute_reply.started": "2025-04-19T17:04:42.620808Z",
          "shell.execute_reply": "2025-04-19T17:05:24.507523Z"
        },
        "id": "_z0X4q08FXgn",
        "outputId": "1d609da0-c3f0-470a-f78a-437f9bb0ab3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "--2025-04-19 17:04:42--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.216.207, 172.217.203.207, 173.194.217.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.216.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3816687935 (3.6G) [application/zip]\nSaving to: ‘nature_12K.zip’\n\nnature_12K.zip      100%[===================>]   3.55G   252MB/s    in 14s     \n\n2025-04-19 17:04:56 (262 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Extraction of iNaturalist data\n",
        "class INatSplitLoader:\n",
        "    \"\"\"\n",
        "    Loads the iNaturalist data, performs a manual per‐class 80/20 split\n",
        "    (without external libs), and exposes PyTorch DataLoaders.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_root: str,\n",
        "        test_root: str,\n",
        "        img_size: tuple = (256, 256),\n",
        "        batch_size: int = 32,\n",
        "        num_workers: int = 2,\n",
        "    ):\n",
        "\n",
        "        # Preprocessing on data\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                 (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.train_root = train_root\n",
        "        self.num_workers = num_workers\n",
        "        self.test_root = test_root\n",
        "\n",
        "    def build(self):\n",
        "        # 1. Load the complete training folder\n",
        "        full_ds = datasets.ImageFolder(self.train_root, transform=self.transform)\n",
        "\n",
        "        # 2. Group indices by class label\n",
        "        class_to_idxs = {}\n",
        "        for idx, (_, lbl) in enumerate(full_ds.samples):\n",
        "            class_to_idxs.setdefault(lbl, []).append(idx)\n",
        "\n",
        "        # 3. For each class, shuffle and split 80/20\n",
        "        train_idxs, val_idxs = [], []\n",
        "        for lbl, idxs in class_to_idxs.items():\n",
        "            # shuffle in‐place\n",
        "            random.shuffle(idxs)\n",
        "            split = int(len(idxs)*0.8)\n",
        "            train_idxs += idxs[:split]\n",
        "            val_idxs   += idxs[split:]\n",
        "\n",
        "        # 4. Load test set untouched\n",
        "        self.test_ds = datasets.ImageFolder(self.test_root, transform=self.transform)\n",
        "\n",
        "        # 5. Build Subsets\n",
        "        self.val_ds   = Subset(full_ds, val_idxs)\n",
        "        self.train_ds = Subset(full_ds, train_idxs)\n",
        "\n",
        "\n",
        "\n",
        "    def get_loaders(self):\n",
        "        \"\"\"\n",
        "        Returns (train_loader, val_loader, test_loader).\n",
        "        Exact same loader settings as before so performance is unaffected.\n",
        "        \"\"\"\n",
        "        test_loader = DataLoader(\n",
        "            self.test_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            self.val_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T16:26:32.533478Z",
          "iopub.execute_input": "2025-04-19T16:26:32.533793Z",
          "iopub.status.idle": "2025-04-19T16:26:32.542737Z",
          "shell.execute_reply.started": "2025-04-19T16:26:32.533766Z",
          "shell.execute_reply": "2025-04-19T16:26:32.54192Z"
        },
        "id": "PKwyX-nYFXgp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Part2: CNN class module for INaturalist dataset - includes function for training\n",
        "\n",
        "class INatCustomCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A configurable CNN tailored for classifying iNaturalist images.\n",
        "    Includes its own training routine with mixed-precision support.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Tuple[int, int, int],\n",
        "        conv_channels: List[int],\n",
        "        kernel_sizes: List[int],\n",
        "        fc_units: int,\n",
        "        act_fn: nn.Module,\n",
        "        use_batchnorm: bool,\n",
        "        drop_p: float,\n",
        "        opt_type: str,\n",
        "        learning_rate: float,\n",
        "        l2_reg: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.feature_extractor = self._build_conv_layers(input_shape[0], conv_channels, kernel_sizes, use_batchnorm, act_fn)\n",
        "        self.flatten_dim = self._infer_flat_dim(input_shape)\n",
        "\n",
        "        self.optimizer = self._configure_optimizer(opt_type, learning_rate, l2_reg)\n",
        "\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.flatten_dim, fc_units),\n",
        "            act_fn,\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(fc_units, 10)  # Output layer for 10 classes\n",
        "        )\n",
        "\n",
        "    def _infer_flat_dim(self, input_shape):\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, *input_shape)\n",
        "            out = self.feature_extractor(dummy_input)\n",
        "        return out.view(out.size(0), -1).size(1)\n",
        "\n",
        "    def _configure_optimizer(self, opt_type, lr, weight_decay):\n",
        "        optimizers = {\n",
        "            'adam': lambda p: torch.optim.Adam(p, lr=lr, weight_decay=weight_decay),\n",
        "            'sgd': lambda p: torch.optim.SGD(p, lr=lr, weight_decay=weight_decay, momentum=0.9),\n",
        "            'rmsprop': lambda p: torch.optim.RMSprop(p, lr=lr, weight_decay=weight_decay),\n",
        "            'nadam': lambda p: torch.optim.NAdam(p, lr=lr, weight_decay=weight_decay),\n",
        "        }\n",
        "        return optimizers[opt_type](self.parameters())\n",
        "\n",
        "    def _build_conv_layers(self, in_channels, channel_list, kernel_list, use_bn, act_fn):\n",
        "        layers = []\n",
        "        for out_channels, k_size in zip(channel_list, kernel_list):\n",
        "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=k_size))\n",
        "            if use_bn:\n",
        "                layers.append(nn.BatchNorm2d(out_channels))\n",
        "            layers.append(act_fn)\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
        "            in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        features = self.feature_extractor(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "    def training_model(self, train_loader, val_loader, epochs, device, log_fn=lambda m: None):\n",
        "        \"\"\"\n",
        "        Runs the training loop with validation per epoch. Supports AMP (automatic mixed precision).\n",
        "        \"\"\"\n",
        "        self.to(device)\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        for ep in range(1, epochs + 1):\n",
        "            self.train()\n",
        "            epoch_loss, correct_preds, total_seen = 0, 0, 0\n",
        "\n",
        "            for imgs, targets in train_loader:\n",
        "                imgs, targets = imgs.to(device), targets.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = self(imgs)\n",
        "                    loss = self.loss_fn(outputs, targets)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(self.optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                correct_preds += (outputs.argmax(1) == targets).sum().item()\n",
        "                total_seen += targets.size(0)\n",
        "\n",
        "            train_acc = 100 * correct_preds / total_seen\n",
        "            train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "            # Validation phase\n",
        "            self.eval()\n",
        "            val_loss, val_correct, val_total = 0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for imgs, targets in val_loader:\n",
        "                    imgs, targets = imgs.to(device), targets.to(device)\n",
        "                    preds = self(imgs)\n",
        "                    loss = self.loss_fn(preds, targets)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_correct += (preds.argmax(1) == targets).sum().item()\n",
        "                    val_total += targets.size(0)\n",
        "\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            print(f\"Epoch {ep}/{epochs} | \"\n",
        "                  f\"training Loss: {train_loss:.4f}, training accuracy: {train_acc:.2f}% | \"\n",
        "                  f\"validation Loss: {avg_val_loss:.4f}, validation accuracy Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            log_fn({\n",
        "                'validation_loss': avg_val_loss,\n",
        "                'validation_accuracy': val_acc,\n",
        "                'epoch': ep,\n",
        "                'train_accuracy': train_acc,\n",
        "                'train_loss': train_loss,\n",
        "            })\n",
        "\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T17:07:27.008771Z",
          "iopub.execute_input": "2025-04-19T17:07:27.009533Z",
          "iopub.status.idle": "2025-04-19T17:07:27.024492Z",
          "shell.execute_reply.started": "2025-04-19T17:07:27.009505Z",
          "shell.execute_reply": "2025-04-19T17:07:27.023792Z"
        },
        "id": "E-kS78MXFXgu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: W&B Hyperparameter Sweep Setup\n",
        "\n",
        "def configure_sweep():\n",
        "    sweep_dict = {\n",
        "        'method': 'bayes',\n",
        "        'name': 'dl_a2_part1',\n",
        "        'metric': {\n",
        "            'goal': 'maximize',\n",
        "            'name': 'validation_accuracy'\n",
        "        },\n",
        "        'parameters': {\n",
        "            'batch_norm': {\n",
        "                'values': ['true', 'false']\n",
        "            },\n",
        "            'batch_size': {\n",
        "                'values': [64, 32]\n",
        "            },\n",
        "            'filter_sizes': {\n",
        "                'values': [[3]*5, [3,5,3,5,3], [5]*5, [5,3,5,3,5]]\n",
        "            },\n",
        "            'num_filters': {\n",
        "                'values': [\n",
        "                    [32, 64, 128, 256, 512],\n",
        "                    [512, 256, 128, 64, 32],\n",
        "                    [32]*5,\n",
        "                    [32, 64, 64, 128, 128],\n",
        "                    [128, 128, 64, 64, 32],\n",
        "                ]\n",
        "            },\n",
        "            'learning_rate': {\n",
        "                'values': [1e-3, 1e-4]\n",
        "            },\n",
        "            'data_aug': {\n",
        "                'values': ['true', 'false']\n",
        "            },\n",
        "            'weight_decay': {\n",
        "                'values': [0.0, 0.0005, 0.5]\n",
        "            },\n",
        "            'dropout': {\n",
        "                'values': [0.0, 0.2, 0.4]\n",
        "            },\n",
        "            'activation': {\n",
        "                'values': ['relu', 'elu', 'silu']\n",
        "            },\n",
        "            'optimiser': {\n",
        "                'values': ['nadam', 'adam', 'rmsprop']\n",
        "            },\n",
        "            'dense_layer': {\n",
        "                'values': [128, 256, 512]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return sweep_dict\n",
        "\n",
        "\n",
        "def run_sweep_experiment(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        cfg = wandb.config\n",
        "        run_name = f\"{cfg.batch_size}_{cfg.activation}_{cfg.optimiser}_{cfg.num_filters}\"\n",
        "\n",
        "        wandb.run.name = run_name\n",
        "        wandb.run.save()\n",
        "\n",
        "        activation_fn_map = {\n",
        "            'relu': nn.ReLU(),\n",
        "            'elu': nn.ELU(),\n",
        "            'silu': nn.SELU()\n",
        "        }\n",
        "\n",
        "        act_fn = activation_fn_map[cfg.activation]\n",
        "\n",
        "        loader = INatSplitLoader(\n",
        "            train_root='inaturalist_12K/train',\n",
        "            test_root='inaturalist_12K/val',\n",
        "            img_size=(256, 256),\n",
        "            batch_size=cfg.batch_size\n",
        "        )\n",
        "        loader.build()\n",
        "        train_loader, val_loader, _ = loader.get_loaders()\n",
        "\n",
        "        model = INatCustomCNN(\n",
        "            input_shape=(3, 256, 256),\n",
        "            conv_channels=cfg.num_filters,\n",
        "            kernel_sizes=cfg.filter_sizes,\n",
        "            fc_units=cfg.dense_layer,\n",
        "            act_fn=act_fn,\n",
        "            use_batchnorm=(cfg.batch_norm.lower() == 'true'),\n",
        "            drop_p=cfg.dropout,\n",
        "            opt_type=cfg.optimiser,\n",
        "            learning_rate=cfg.learning_rate,\n",
        "            l2_reg=cfg.weight_decay\n",
        "        )\n",
        "\n",
        "        model.training_model(\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=10,\n",
        "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "            log_fn=wandb.log\n",
        "        )\n",
        "\n",
        "\n",
        "# Trigger sweep\n",
        "sweep_identifier = wandb.sweep(configure_sweep(), project='cs24m020_dl_a2_sweep1')\n",
        "wandb.agent(sweep_identifier, function=run_sweep_experiment, count=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T17:07:35.957693Z",
          "iopub.execute_input": "2025-04-19T17:07:35.958422Z",
          "iopub.status.idle": "2025-04-19T17:17:39.613755Z",
          "shell.execute_reply.started": "2025-04-19T17:07:35.958395Z",
          "shell.execute_reply": "2025-04-19T17:17:39.61296Z"
        },
        "id": "WcG9TntIFXgx",
        "outputId": "f06d5408-e9b8-4ee4-d729-832840d5990c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Create sweep with ID: 2wdeix3t\nSweep URL: https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/sweeps/2wdeix3t\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ii9k74kb with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: false\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_aug: true\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_layer: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [5, 3, 5, 3, 5]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [32, 64, 64, 128, 128]\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.19.6"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_170741-ii9k74kb</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/runs/ii9k74kb' target=\"_blank\">firm-sweep-1</a></strong> to <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/sweeps/2wdeix3t' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/sweeps/2wdeix3t</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/sweeps/2wdeix3t' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/sweeps/2wdeix3t</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/runs/ii9k74kb' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/runs/ii9k74kb</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_31/4230505403.py:73: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n/tmp/ipykernel_31/4230505403.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10 | training Loss: 2.6488, training accuracy: 10.84% | validation Loss: 2.3027, validation accuracy Acc: 10.00%\nEpoch 2/10 | training Loss: 2.3030, training accuracy: 10.03% | validation Loss: 2.3028, validation accuracy Acc: 10.00%\nEpoch 3/10 | training Loss: 2.3030, training accuracy: 9.58% | validation Loss: 2.3027, validation accuracy Acc: 10.00%\nEpoch 4/10 | training Loss: 2.3031, training accuracy: 9.80% | validation Loss: 2.3028, validation accuracy Acc: 10.00%\nEpoch 5/10 | training Loss: 2.3029, training accuracy: 9.76% | validation Loss: 2.3027, validation accuracy Acc: 10.00%\nEpoch 6/10 | training Loss: 2.3030, training accuracy: 10.30% | validation Loss: 2.3027, validation accuracy Acc: 10.00%\nEpoch 7/10 | training Loss: 2.3030, training accuracy: 9.90% | validation Loss: 2.3027, validation accuracy Acc: 10.00%\nEpoch 8/10 | training Loss: 2.3030, training accuracy: 10.46% | validation Loss: 2.3026, validation accuracy Acc: 10.00%\nEpoch 9/10 | training Loss: 2.3030, training accuracy: 9.39% | validation Loss: 2.3027, validation accuracy Acc: 10.00%\nEpoch 10/10 | training Loss: 2.3029, training accuracy: 9.60% | validation Loss: 2.3027, validation accuracy Acc: 10.00%\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>█▄▂▃▃▅▃▆▁▂</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▅▆▅█▅▄▄▁▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>9.6012</td></tr><tr><td>train_loss</td><td>2.30289</td></tr><tr><td>validation_accuracy</td><td>10</td></tr><tr><td>validation_loss</td><td>2.30274</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">32_silu_rmsprop_[32, 64, 64, 128, 128]</strong> at: <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/runs/ii9k74kb' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1/runs/ii9k74kb</a><br> View project at: <a href='https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1' target=\"_blank\">https://wandb.ai/karekargrishma1234-iit-madras-/cs24m020_dl_a2_sweep1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20250419_170741-ii9k74kb/logs</code>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "X2qXRP4RFXg0"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}